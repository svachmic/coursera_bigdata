{"cells":[{"cell_type":"markdown","source":["# Machine Learning With Big Data\n## by University of California, San Diego\n\n### Week 4\n\nThe script is set up to run Kmeans for k=1 clusters. Observe the SSE that is printed out by the script.\n\n(Note: In fact if you look at the cluster centers, using my_kmmodel.clusterCenters, you get almost the same values used to create the data. You can use the cluster center coordinate points as a kind of summary of the data. In some cases the cluster centers could be labeled to serve as descriptions of the data.)\n\n#### Change the number of clusters to k=4 and rerun the training command and get the SSE (or rerun the whole script).\n\n* Observe the approximate SSE for k=4.\n* Try getting summary statistics on the RDD of random data.\n* Take note of standard deviation values. If you square each value you get the variance for each dimension.\n* Compare these stats results to Kmeans SSE when k=3.\n\nObserve the relationship between SSE when k=3 and the variance of each dimension. It's not obvious but write down the numbers for the quiz."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.clustering import KMeans\nfrom pyspark.mllib.random import RandomRDDs\n\n# let's generate random class data, add in a cluster center to random 2D points\n# use default num of partitions, or use a definte number to make it so that the union\n# will have samples across clusters\nc1_v=RandomRDDs.normalVectorRDD(sc,20,2,numPartitions=2,seed=1L).map(lambda v:np.add([1,5],v))\nc2_v=RandomRDDs.normalVectorRDD(sc,16,2,numPartitions=2,seed=2L).map(lambda v:np.add([5,1],v))\nc3_v=RandomRDDs.normalVectorRDD(sc,12,2,numPartitions=2,seed=3L).map(lambda v:np.add([4,6],v))\n\n# concatenate 2 RDDs with .union(other) function\nc12=c1_v.union(c2_v)\nmy_data=c12.union(c3_v) # this now has all points, as RDD\n\n# change the number of clusters here\nmy_kmmodel = KMeans.train(my_data,k=1,\n               maxIterations=20,runs=1,\n               initializationMode='k-means||',seed=10L)\n\n# try: help(KMeans.train)  to see parameter options\n# k is the number of desired clusters.\n# maxIterations is the maximum number of iterations to run.\n# initializationMode specifies either random initialization or initialization via k-means||.\n# runs is the number of times to run the k-means algorithm (k-means is not guaranteed to find \n# a globally optimal solution, and when run multiple times on a given dataset, the algorithm returns the best clustering result).\n# initializationSteps determines the number of steps in the k-means|| algorithm.\n# epsilon determines the distance threshold within which we consider k-means to have converged.\n\n# type dir(my_kmmodel) to see functions available on the cluster results object\n\n# The computeCost function might not be available on your cloudera vm,\n# spark mlllib, it computes the Sum Squared Error: my_kmmodel.computeCost(my_data)  \n\n# This does the same thing as computeCost, and gives an example of coding a metric\n# get sse of a point to the center of the cluster it's assigned to\ndef getsse(point):\n    this_center = my_kmmodel.centers[my_kmmodel.predict(point)]\n           #for this point get it's clustercenter\n    return (sum([x**2 for x in (point - this_center)])) \n\nmy_sse=my_data.map(getsse).collect()  #collect list of sse of each pt to its center\n\nprint np.array(my_sse).mean()\nprint my_data.stats()"],"metadata":{},"outputs":[],"execution_count":2}],"metadata":{"name":"kmeans","notebookId":4239978241248127},"nbformat":4,"nbformat_minor":0}
